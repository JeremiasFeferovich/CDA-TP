# -*- coding: utf-8 -*-
"""Segund_Entregable_CDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IpmlBXcqsL3dJammAqyDrG4r9HJCW7iP

# Ciencia de datos aplicada (ITBA): Segundo entregable

**Equipo:** Ian Bernasconi, Jeremias Feferovich

**Nombre del proyecto**: Batata Real State

Los datos fueron obtenidos haciendo scrapping de https://properati.com.ar/.

Inicialmente quisimos obtenerlos de https://www.zonaprop.com.ar/, pero tuvimos complicaciones por c√≥mo maneja zonaprop la paginaci√≥n, lo que hizo que tuvi√©ramos muchos datos repetidos.

Al realizar el scrapper para properati, iterando con el url base https://www.properati.com.ar/s/capital-federal/venta, vemos que 69.269 propiedades.
Filtrando por tipo de propiedad Casa o Departamento, para excluir otros tipos como Locales o Cocheras, nos quedan 49959.

Sin embargo, al correr el scrapper, nos encontramos con que a partir de la p√°gina 168 ya no obten√≠amos resultados. Con 30 propiedades por p√°gina, esto nos limitaba a alrededor de 5000 propiedades. Para solucionar esto, scrappeamos por separado para cada barrio de la ciudad, dividiendo Palermo en Palermo Chico, Palermo Viejo, Palermo Hollywood y Palermo Soho, ya que se pasaba del l√≠mite de 5000 propiedades.

Como comentario adicional, para realizar el scrapper se utiliz√≥ el MCP de Playwright para facilitar el reconocimiento de la estructura de las p√°ginas a scrappear.

El c√≥digo del scrapper puede verse en https://github.com/JeremiasFeferovich/CDA-TP

### üßæ 1. Importaci√≥n y carga de librer√≠as
"""

!pip install category_encoders

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import category_encoders as ce
import io
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from google.colab import files

# Configuraciones de estilo
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 6)

"""### üßæ 2. Carga de datos"""

file_id = "15nLVZdk6KSnKo4TmzjFHlBs6rmkPeFDJ"
url = f'https://drive.google.com/uc?id={file_id}'
df = pd.read_csv(url)
print(df.shape)
print(df.head())

"""#### üóíÔ∏è 2.1 Descripci√≥n del dataset
Este dataset contiene 46 variables sobre propiedades en venta en la Ciudad de Buenos Aires, obtenidas mediante t√©cnicas de web scraping en portales inmobiliarios. Incluye informaci√≥n estructural de las viviendas, caracter√≠sticas adicionales y datos de localizaci√≥n.

Tama√±o del dataset: 49959 registros.

- Variables num√©ricas

  - price: precio de publicaci√≥n de la propiedad .

  - surface_total: superficie total en metros cuadrados.

  - surface_covered: superficie cubierta .

  - bathrooms: cantidad de ba√±os.

  - bedrooms: cantidad de dormitorios.

  - rooms: n√∫mero total de ambientes .

  - area: √°rea de la propiedad.

  - balcony_count: cantidad de balcones.

  - photo_count: cantidad de fotos de la publicaci√≥n.

  - page_number: n√∫mero de p√°gina en el scraping.

  - parking_spaces: cantidad de cocheras.

  - expenses: valor mensual de expensas.

  - total_floors / floor_number: pisos totales y n√∫mero de piso.

- Variables categ√≥ricas

  - currency: tipo de moneda (USD, ARS, etc.).

  - neighborhood / neighborhood_slug / scraped_neighborhood: barrio de la Ciudad de Buenos Aires.

  - location / coordinates / latitude / longitude: informaci√≥n geogr√°fica.

  - full_address: direcci√≥n completa de la propiedad.

  - property_type: tipo de propiedad (departamento, casa, PH, etc.).

  - property_status: estado de la propiedad.

  - orientation: orientaci√≥n del inmueble.

  - labels: etiquetas descriptivas de la publicaci√≥n.

  - title: t√≠tulo del aviso.

  - description: texto descriptivo.

  - detail_url: enlace a la publicaci√≥n.

  - published_date: fecha de publicaci√≥n.

  - publisher: entidad que publica .

  - amenities: listado de amenidades asociadas (ej. ‚Äúpileta, parrilla, seguridad‚Äù).

- Variables booleanas (amenities espec√≠ficas)

  - has_balcony, has_doorman, has_garage, has_grill, has_gym, has_pool, has_security, has_storage, has_sum, has_terrace: indican presencia de determinadas caracter√≠sticas.

- Variables identificadoras

  - property_id: identificador √∫nico de la propiedad.

  - agency_name: nombre de la inmobiliaria.

- Metadatos del scraping

  - page_number, scraping_date: informaci√≥n t√©cnica del proceso de scraping.

- Variable objetivo

  - price: precio de publicaci√≥n (expresado mayormente en USD)

### üîç 3. An√°lisis exploratorio de datos (EDA)

####üöÄ Primer acercamiento al dataset
"""

# Informaci√≥n general
df.info()

""" Gran parte de las variables aparecen cargadas con tipo object. Esto implica la necesidad de realizar un proceso de conversi√≥n de tipos para poder analizarlas.


"""

# Informaci√≥n general
df.describe()

# Valores faltantes
df.isnull().sum()

"""Borrar tuplas que tengan NaN en "price" que es el valor objetivo

"""

df = df.dropna(subset=["price"])

"""Hacemos un chequeo de propiedades repetidas segun property_id:"""

# Vemos el estado inicial
print("Shape inicial:", df.shape)

# Elimina duplicados manteniendo la primera aparici√≥n
df = df.drop_duplicates(subset="property_id", keep="first")

# Luego de eliminar tuplas duplicadas
print("Shape final:", df.shape)

"""Por ende, no hay duplicados en el dataset

####üî¢ Distribucion de Variables numericas
"""

# Distribuciones de variables num√©ricas
cols = ["price", "surface_total", "bathrooms", "bedrooms", "area", "balcony_count"]
df_numeric = df[cols]

df_numeric.hist(bins=10, figsize=(15, 10), layout=(4, 2))
plt.suptitle('Distribuci√≥n de variables num√©ricas')
plt.tight_layout()
plt.show()

"""#### üßä Detecci√≥n de outliers con boxplots


"""

fig, axes = plt.subplots(3, 2, figsize=(15, 10))

columnas = df_numeric.columns

for i, ax in enumerate(axes.flat):
    sns.boxplot(y=df[columnas[i]], ax=ax)
    ax.set_title(columnas[i])

plt.suptitle('Boxplots de variables num√©ricas', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""#### üîó Matriz de correlaci√≥n entre variables num√©ricas"""

plt.figure(figsize=(12, 8))
sns.heatmap(df_numeric.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Matriz de correlaci√≥n")
plt.show()

"""#### üìä Relaci√≥n entre variables: ejemplo surface vs price"""

plt.figure(figsize=(10, 6))
plt.scatter(df["surface_total"], df["price"], alpha=0.3, edgecolor="k", s=20)
plt.title("Distribuci√≥n del precio seg√∫n la superficie")
plt.xlabel("Superficie (m¬≤)")
plt.ylabel("Precio")

# Mejorar eje X
# plt.xscale("log")                # alternativa si prefer√≠s log
plt.xticks(rotation=45)
plt.locator_params(axis="x", nbins=10)

plt.show()

"""### üßº 4. Diagn√≥stico de calidad de datos
- Valores faltantes
  - Se observa una gran cantidad de columnas con valores nulos totales o casi totales, lo que impacta negativamente en la completitud del dataset.
  - Columnas completamente vac√≠as (sin valor alguno): description, expenses, floor_number, orientation, parking_spaces, property_status, surface_covered, total_floors.
  - Columnas con nulos parciales relevantes:
    - area: 14.560 nulos (~29%).
    - bathrooms: 2.138 nulos (~4%).
    - bedrooms: 7.823 nulos (~16%).
    - currency: 504 nulos (~1%).
    - rooms: 49.893 nulos (~99%).
    - surface_total: 14.560 nulos (~29%).
- Outliers
  - La variable price presenta un valor m√°ximo de 3995 millones USD.
  - La desviaci√≥n est√°ndar (~ 17.9 millones) es desproporcionada frente al promedio (~323 mil).
  - En bathrooms y bedrooms, los m√°ximos (280 y 51 respectivamente) son inveros√≠miles para propiedades residenciales y sugieren errores.
  - area / surface_total presentan valores m√°ximos de 970 m¬≤, que podr√≠an ser v√°lidos para propiedades de alta gama, pero conviene verificar su distribuci√≥n.
- Variables que deben ser eliminadas
  - rooms tiene solo 66 valores no nulos y una desviaci√≥n est√°ndar igual a 0, lo que indica falta total de variabilidad. Por lo tanto, no aporta informaci√≥n √∫til.
  - expenses, orientation, floor_number, parging_spaces, property_status, surface_covered y total_floors est√°n completamente vac√≠as.
  - Las columnas agency_name, description, detail_url, photo_count,  published_date, publisher, title no otorgan informaci√≥n relevante
  - Metadatos del scrapping: page_number, property_id (nos sirvi√≥ para borrar duplicados), scraping_date
  - Las columnas full_address, location, neighborhood, neighborhood_slug, scraped_neighborhood otorgan las misma inforamaci√≥n. Nos quedaremos con location y borraremos las dem√°s.
  - Las columnas area y surface_total son id√©nticas por lo que nos quedaremos solo con area.
  - Los valores posibles de la columna amenities son considerados como booleanos en nuevas columnas

### üß™ 5. Transformaciones realizadas

#### Limpieza de columnas
"""

cols_to_drop = [
    'amenities', 'rooms', 'expenses', 'orientation', 'floor_number', 'parking_spaces',
    'property_status', 'surface_covered', 'total_floors',
    'agency_name', 'description', 'detail_url', 'photo_count',
    'published_date', 'publisher', 'title',
    'page_number', 'property_id', 'scraping_date',
    'full_address', 'neighborhood', 'neighborhood_slug', 'scraped_neighborhood' # nos quedamos con location
    'surface_total'  # nos quedamos con 'area'
]

df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True, errors='ignore')

print(f" Total actual de columnas: {df.shape[1]} columnas")

"""#### Valores faltantes"""

cols_imputar = ['currency']

# Mostrar cantidad de nulos antes
print("üîç Nulos antes de imputar:")
for col in cols_imputar:
    print(f" - {col}: {df[col].isna().sum()} nulos")

df['currency'] = df['currency'].fillna('USD')  # default m√°s com√∫n

print("\nNulos despu√©s de imputar:")
for col in cols_imputar:
    print(f" - {col}: {df[col].isna().sum()} nulos restantes")

cols_eliminar_si_nulos = ['area', 'bathrooms', 'bedrooms', 'currency']

# Mostrar cantidad de nulos antes
print("üîç Nulos antes de eliminar:")
for col in cols_eliminar_si_nulos:
    print(f" - {col}: {df[col].isna().sum()} nulos")

# Eliminar filas que tengan nulos en cualquiera de esas columnas
before_drop = df.shape[0]
df = df.dropna(subset=cols_eliminar_si_nulos)
after_drop = df.shape[0]

print(f"\nüßπ Filas eliminadas: {before_drop - after_drop}")
print(f"‚úÖ Total de filas restantes: {after_drop}")

# Verificaci√≥n final
print("\nüîç Nulos despu√©s de eliminar:")
for col in cols_eliminar_si_nulos:
    print(f" - {col}: {df[col].isna().sum()} nulos restantes")

print(f"\nCantidad total de tuplas en el DataFrame: {len(df)}")

# Eliminaci√≥n de filas con valores 0 en columnas clave

cols_sin_ceros = ['area', 'bathrooms', 'bedrooms', 'price']

print("üîç Cantidad de valores 0 antes de eliminar:")
for col in cols_sin_ceros:
    print(f" - {col}: {(df[col] == 0).sum()} filas con 0")

# Eliminar filas que tengan 0 en cualquiera de esas columnas
before_drop = df.shape[0]
df = df[~(df[cols_sin_ceros] == 0).any(axis=1)]
after_drop = df.shape[0]

print(f"\nFilas eliminadas por contener valores 0 en columnas clave: {before_drop - after_drop}")
print(f"Total de filas restantes: {after_drop}")

# Verificaci√≥n
print("\nCantidad de valores 0 despu√©s de eliminar:")
for col in cols_sin_ceros:
    print(f" - {col}: {(df[col] == 0).sum()} filas con 0")

"""#### Trato de Outliers"""

numeric_cols = ['price', 'area', 'bathrooms', 'bedrooms', 'balcony_count']

print("Antes del tratamiento de outliers:\n")
for col in numeric_cols:
    print(f"‚Üí {col}")
    print(f"   Media: {df[col].mean():,.2f}")
    print(f"   Desviaci√≥n est√°ndar: {df[col].std():,.2f}")
    print(f"   M√≠nimo: {df[col].min():,.2f}")
    print(f"   M√°ximo: {df[col].max():,.2f}\n")

"""Se aplica un tratamiento diferenciado de outliers para mantener la coherencia de los datos sin eliminar informaci√≥n √∫til.
- En price:
  - Se filtran precios m√≠nimos sin sentido: eliminar valores extremadamente bajos, precios menores a 10 000 USD.

  - Analizar los precios m√°s altos y compararlos con los percentiles estad√≠sticos para decidir si son razonables o deben recortarse.


"""

min_valid_price = 10000  # umbral m√≠nimo razonable en USD
before_min_filter = len(df)
df = df[df['price'] >= min_valid_price]
after_min_filter = len(df)

print(f" Filtro inferior aplicado a 'price' (< {min_valid_price:,} USD)")
print(f" Filas eliminadas: {before_min_filter - after_min_filter}")
print(f" Total de filas restantes: {after_min_filter}")

# --- 2Ô∏è‚É£ Chequeo de valores m√°ximos (outliers altos) ---
q1 = df['price'].quantile(0.25)
q3 = df['price'].quantile(0.75)
iqr = q3 - q1
upper_limit = q3 + 1.5 * iqr

print("\nüìä An√°lisis de valores altos:")
print(f" - Q3 (percentil 75): {q3:,.2f}")
print(f" - L√≠mite superior (Q3 + 1.5*IQR): {upper_limit:,.2f}")
print(f" - M√°ximo actual: {df['price'].max():,.2f}")

"""Revisando seg√∫n precio m√°ximo:"""

# Revisi√≥n de las propiedades m√°s caras
top_prices = df.nlargest(10, 'price')[['price', 'area', 'bedrooms', 'bathrooms', 'location', 'currency']]
print("\n Propiedades con los precios m√°s altos:\n")
print(top_prices)

"""Vamos a suponer que los cuatro primeros valores son errores en el scarapping o de carga de las propiedades. Eliminaremos todo lo que est√© por encima de 12 millones USD."""

max_valid_price = 12000000
before = len(df)
df = df[df['price'] <= max_valid_price]
after = len(df)

print(f"Se eliminaron {before - after} propiedades con precio > {max_valid_price:,} USD")
print(f"\nNuevo m√°ximo de price: {df['price'].max():,.2f} USD")

"""- En area, se eliminan superficies irreales (menores a 30 m¬≤)."""

menos_20 = (df['area'] < 30).sum()

print(f" Propiedades con √°rea menor a 30 m¬≤: {menos_20}")

total = len(df)
porcentaje = (menos_20 / total) * 100
print(f" Propiedades con √°rea < 30 m¬≤: {menos_20} ({porcentaje:.2f}% del total)")

# Recorte para √°rea (filtrado por l√≠mites razonables: menos de 2% total)
min_area = 30
df = df[(df['area'] >= min_area)]
print(f" Recorte aplicado a 'area' | Rango final: {df['area'].min():,.2f} - {df['area'].max():,.2f}")

# Mostrar las 30 propiedades con mayor √°rea
top_areas = df.nlargest(30, 'area')[['area', 'price', 'bedrooms', 'bathrooms']]
print("üè† Top 30 propiedades con mayor √°rea:\n")
print(top_areas)

# Calcular estad√≠sticas generales
q1 = df['area'].quantile(0.25)
q3 = df['area'].quantile(0.75)
iqr = q3 - q1
upper_limit = q3 + 1.5 * iqr

print("\n Estad√≠sticas de √°rea:")
print(f" - Percentil 75 (Q3): {q3:.2f} m¬≤")
print(f" - L√≠mite superior para outliers (Q3 + 1.5*IQR): {upper_limit:.2f} m¬≤")
print(f" - M√°ximo real observado: {df['area'].max():.2f} m¬≤")

# Evaluar si el m√°ximo es un outlier real o razonable
if df['area'].max() > upper_limit:
    print("\n El valor m√°ximo supera el l√≠mite estad√≠stico.")
    print("Sin embargo, considerando el contexto inmobiliario, puede representar propiedades grandes y no un error.")
else:
    print("\n El valor m√°ximo est√° dentro del rango esperado.")

"""
- En bathrooms y bedrooms, se limitan los valores m√°ximos a rangos habitacionales razonables. Dormitorios a 11 y ba√±os a 8."""

max_bathrooms = (df['bathrooms'] > 8).sum()

print(f" Propiedades con m√°s de 8 ba√±os: {max_bathrooms}")

total = len(df)
porcentaje = (max_bathrooms / total) * 100
print(f" Propiedades con m√°s de 8 ba√±os:: {max_bathrooms} ({porcentaje:.2f}% del total)")

before_drop = len(df)
df = df[df['bathrooms'] <= 8]
after_drop = len(df)

max_bedrooms = (df['bedrooms'] > 11).sum()

print(f" Propiedades con m√°s de 11 dormitorios: {max_bedrooms}")

total = len(df)
porcentaje = (max_bedrooms / total) * 100
print(f" Propiedades con m√°s de 11 dormitorios: {max_bedrooms} ({porcentaje:.2f}% del total)")

before_drop = len(df)
df = df[df['bedrooms'] <= 11]
after_drop = len(df)

"""Cantidad de registros totales luego de las transformacionaes realizadas:"""

df.count()

plt.figure(figsize=(10, 6))
plt.scatter(df["area"], df["price"], alpha=0.3, edgecolor="k", s=20)
plt.title("Distribuci√≥n del precio seg√∫n la superficie")
plt.xlabel("Area (m¬≤)")
plt.ylabel("Precio")
plt.xticks(rotation=45)
plt.locator_params(axis="x", nbins=10)

plt.show()

"""#### Conversi√≥n de tipos

"""

import re

def parse_coordinates(coord):
    if isinstance(coord, str):
        match = re.findall(r'-?\d+\.\d+', coord)
        if len(match) == 2:
            return float(match[0]), float(match[1])
    return (np.nan, np.nan)

print("Antes del parseo:")
print(df[['coordinates', 'latitude', 'longitude']].head(5))

#Parseo
df[['latitude', 'longitude']] = df['coordinates'].apply(
    lambda x: pd.Series(parse_coordinates(x))
)

# Verificar
print(" Conversi√≥n corregida desde 'coordinates':")
print(df[['latitude', 'longitude']].dtypes)
print("\nNulos despu√©s del parseo corregido:")
print(f" - latitude: {df['latitude'].isna().sum()}")
print(f" - longitude: {df['longitude'].isna().sum()}")

# Ver muestra
print("\n Ejemplo de coordenadas parseadas correctamente:")
print(df[['coordinates', 'latitude', 'longitude']].head(5))
df.drop(columns=['coordinates'], inplace=True)

"""#### Normalizaci√≥n de texto

"""

df['location'] = df['location'].str.lower().str.strip()
df['currency'] = df['currency'].str.upper().str.strip()
df['property_type'] = df['property_type'].str.lower().str.strip()

"""#### Generaci√≥n de nuevas variables"""

import ast

# Convertir string de lista a lista real
df['labels'] = df['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])

# Crear variables para los diferntes tipos de labels
df['is_destacado'] = df['labels'].apply(lambda x: 'DESTACADO' in x)
df['is_nuevo'] = df['labels'].apply(lambda x: 'NUEVO' in x)

df.drop(columns=['labels'], inplace=True)

df['property_type'].unique()

# --- Crear columnas binarias para tipos de propiedad clave ---
df['property_type'] = df['property_type'].str.lower().str.strip()

df['is_departamento'] = df['property_type'].apply(lambda x: 1 if 'departamento' in x else 0)
df['is_casa'] = df['property_type'].apply(lambda x: 1 if 'casa' in x else 0)

print("\nColumnas creadas correctamente:")
print(df[['is_departamento', 'is_casa']].head(5))

df.drop(columns=['property_type'], inplace=True)

"""#### Normalizacion y Estandariacion de los datos"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler_minmax = MinMaxScaler()
scaler_std = StandardScaler()

# Crear copias escaladas
df_norm = df.copy()

col_norm = ['area']

df_norm[col_norm] = scaler_minmax.fit_transform(df_norm[col_norm])

df_norm.head()

""" ### ‚úçÔ∏è 6. Reflexi√≥n final

En este proceso de diagn√≥stico y transformaci√≥n de los datos, se tomaron las siguientes decisiones:

- **Eliminaci√≥n de columnas irrelevantes o con alta proporci√≥n de nulos:** Se identificaron y eliminaron columnas que no aportaban informaci√≥n √∫til para el an√°lisis o modelado, como identificadores de scraping, URLs, y columnas con casi todos sus valores nulos (ej. `description`, `expenses`, `rooms`). Esto permiti√≥ reducir la dimensionalidad del dataset y enfocarse en variables m√°s significativas.

- **Manejo de valores faltantes:** Para columnas clave como `area`, `bathrooms` y `bedrooms`, se opt√≥ por eliminar las filas con valores nulos, ya que representaban una parte menor del dataset y su imputaci√≥n podr√≠a introducir sesgos. Para la moneda (`currency`), se imputaron los nulos con 'USD' al ser el valor m√°s frecuente.

- **Tratamiento de Outliers:** Se realiz√≥ un an√°lisis detallado de los outliers en variables num√©ricas (`price`, `area`, `bathrooms`, `bedrooms`). Se definieron umbrales basados en el contexto inmobiliario y en percentiles para filtrar valores extremos que parec√≠an errores de carga o scraping, buscando mantener la consistencia de los datos sin eliminar informaci√≥n relevante sobre propiedades de alto valor o gran tama√±o.

- **Conversi√≥n y normalizaci√≥n de tipos de datos:** Se transformaron columnas con formato inconsistente (ej. `coordinates`) a tipos num√©ricos (`latitude`, `longitude`) para facilitar su an√°lisis. Se normaliz√≥ el texto en columnas categ√≥ricas (`location`, `currency`, `property_type`) para asegurar la uniformidad.

- **Creaci√≥n de nuevas variables:** Se generaron variables binarias a partir de la columna `labels` (`is_destacado`, `is_nuevo`) y `property_type` (`is_departamento`, `is_casa`) para capturar caracter√≠sticas importantes de las propiedades de manera estructurada.

- **Normalizaci√≥n de variables num√©ricas:** Se aplic√≥ MinMaxScaler a la columna `area` para normalizar su escala, prepar√°ndola para futuros modelos que puedan ser sensibles a la magnitud de las variables de entrada.

**Dificultades encontradas:**

La principal dificultad fue la gran cantidad de valores faltantes y la presencia de outliers significativos en variables clave como el precio y la superficie. Esto requiri√≥ un an√°lisis cuidadoso para decidir c√≥mo tratarlos sin perder informaci√≥n valiosa o introducir sesgos. La inconsistencia en el formato de las coordenadas tambi√©n represent√≥ un desaf√≠o inicial.

**Siguientes pasos proyectados:**

1.  **An√°lisis m√°s profundo de variables categ√≥ricas:** Explorar la distribuci√≥n de las propiedades por `location` y otras variables categ√≥ricas restantes.
2.  **Creaci√≥n de columnas adicionales:** Considerar la creaci√≥n de nuevas variables, como el precio por metro cuadrado (`price_per_sqm`), o la distancia a puntos de inter√©s a partir de las coordenadas.
3.  **An√°lisis de la variable objetivo (`price`):** Realizar una exploraci√≥n m√°s profunda de la distribuci√≥n del precio despu√©s del tratamiento de outliers y considerar posibles transformaciones (ej. logar√≠tmica) si la distribuci√≥n es muy sesgada.
4. An√°lisis geoespacial de coordenadas (latitude, longitude): Realizar un estudio de patrones espaciales para identificar zonas preferenciales, √°reas tur√≠sticas o de alto valor comercial, analizando c√≥mo la ubicaci√≥n geogr√°fica influye en el precio y la demanda de propiedades.
5.  **Selecci√≥n de caracter√≠sticas:** Evaluar la relevancia de las variables restantes para la predicci√≥n del precio.
6.  **Modelado:** Implementar un modelo de predicci√≥n del precio
7.  **Evaluaci√≥n del modelo:** Medir el rendimiento del modelo utilizando m√©tricas adecuadas.
"""