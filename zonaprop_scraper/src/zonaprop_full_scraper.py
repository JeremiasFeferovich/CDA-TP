#!/usr/bin/env python3

import json
import csv
import re
import time
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging

# SeleniumBase imports
from seleniumbase import SB


class ZonaPropFullScraper:
    """
    Scraper completo de ZonaProp usando CDP Mode
    Maneja m√∫ltiples p√°ginas con extracci√≥n de datos reales
    """
    
    def __init__(self, headless: bool = False, incognito: bool = True, delay: float = 3.0):
        """
        Inicializa el scraper completo
        
        Args:
            headless: Ejecutar en modo headless
            incognito: Usar modo inc√≥gnito
            delay: Delay entre p√°ginas en segundos
        """
        self.headless = headless
        self.incognito = incognito
        self.delay = delay
        self.scraped_data = []
        
        # Configurar logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'scraper_full_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        self.logger.info("ZonaProp Full Scraper inicializado con CDP Mode")
    
    def scrape_multiple_pages(self, start_page: int = 1, max_pages: int = 5, 
                            properties_per_page: int = 25) -> List[Dict[str, Any]]:
        """
        Scraper principal para m√∫ltiples p√°ginas
        
        Args:
            start_page: P√°gina inicial
            max_pages: N√∫mero m√°ximo de p√°ginas
            properties_per_page: Propiedades por p√°gina (default: 25, m√°ximo en ZonaProp)
            
        Returns:
            Lista de todas las propiedades extra√≠das
        """
        self.logger.info(f"Iniciando scraping completo: p√°ginas {start_page} a {start_page + max_pages - 1}")
        
        total_extracted = 0
        failed_pages = []
        
        with SB(uc=True, test=True, incognito=self.incognito, headless=self.headless) as sb:
            
            for page_num in range(start_page, start_page + max_pages):
                try:
                    self.logger.info(f"üîÑ Procesando p√°gina {page_num}/{start_page + max_pages - 1}")
                    
                    # Navegar a la p√°gina
                    success = self._navigate_to_page(sb, page_num)
                    if not success:
                        self.logger.error(f"‚ùå Error navegando a p√°gina {page_num}")
                        failed_pages.append(page_num)
                        continue
                    
                    # Extraer propiedades
                    page_properties = self._extract_page_properties(sb, page_num, properties_per_page)
                    
                    if page_properties:
                        self.scraped_data.extend(page_properties)
                        total_extracted += len(page_properties)
                        self.logger.info(f"‚úÖ P√°gina {page_num}: {len(page_properties)} propiedades extra√≠das")
                        self.logger.info(f"üìä Total acumulado: {total_extracted} propiedades")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è  P√°gina {page_num}: No se extrajeron propiedades")
                        failed_pages.append(page_num)
                    
                    # Delay entre p√°ginas (excepto la √∫ltima)
                    if page_num < start_page + max_pages - 1:
                        self.logger.info(f"‚è≥ Esperando {self.delay}s antes de la siguiente p√°gina...")
                        time.sleep(self.delay)
                    
                except KeyboardInterrupt:
                    self.logger.info("üõë Scraping interrumpido por el usuario")
                    break
                except Exception as e:
                    self.logger.error(f"‚ùå Error inesperado en p√°gina {page_num}: {e}")
                    failed_pages.append(page_num)
                    continue
        
        # Resumen final
        self.logger.info(f"üèÅ Scraping completado:")
        self.logger.info(f"   üìä Total extra√≠do: {total_extracted} propiedades")
        self.logger.info(f"   ‚úÖ P√°ginas exitosas: {max_pages - len(failed_pages)}/{max_pages}")
        if failed_pages:
            self.logger.warning(f"   ‚ùå P√°ginas fallidas: {failed_pages}")
        
        return self.scraped_data
    
    def _navigate_to_page(self, sb, page_number: int) -> bool:
        """
        Navega a una p√°gina espec√≠fica usando CDP Mode
        
        Args:
            sb: Instancia de SeleniumBase
            page_number: N√∫mero de p√°gina
            
        Returns:
            True si la navegaci√≥n fue exitosa
        """
        try:
            # Construir URL
            if page_number == 1:
                url = "https://www.zonaprop.com.ar/departamentos-venta.html"
            else:
                url = f"https://www.zonaprop.com.ar/departamentos-venta-pagina-{page_number}.html"
            
            self.logger.info(f"üåê Navegando con CDP Mode: {url}")
            
            # Activar CDP Mode para bypass autom√°tico de Cloudflare
            sb.activate_cdp_mode(url)
            time.sleep(2)
            
            # Verificar t√≠tulo
            title = sb.get_title()
            self.logger.debug(f"T√≠tulo p√°gina {page_number}: {title}")
            
            # Verificar si Cloudflare fue evadido
            if "just a moment" in title.lower():
                self.logger.error(f"üîí Cloudflare detectado en p√°gina {page_number}")
                return False
            
            # Verificar que hay propiedades
            try:
                sb.wait_for_element_visible('[data-qa="posting PROPERTY"]', timeout=10)
                properties_count = len(sb.find_elements('[data-qa="posting PROPERTY"]'))
                
                if properties_count > 0:
                    self.logger.info(f"‚úÖ P√°gina {page_number} cargada: {properties_count} propiedades detectadas")
                    return True
                else:
                    self.logger.warning(f"‚ö†Ô∏è  P√°gina {page_number}: No se detectaron propiedades")
                    return False
                    
            except Exception as e:
                self.logger.error(f"‚ùå Error verificando propiedades en p√°gina {page_number}: {e}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Error navegando a p√°gina {page_number}: {e}")
            return False
    
    def _extract_page_properties(self, sb, page_number: int, max_properties: int) -> List[Dict[str, Any]]:
        """
        Extrae propiedades de la p√°gina actual usando JavaScript + Regex
        
        Args:
            sb: Instancia de SeleniumBase
            page_number: N√∫mero de p√°gina actual
            max_properties: M√°ximo de propiedades a extraer
            
        Returns:
            Lista de propiedades extra√≠das
        """
        try:
            self.logger.info(f"üîç Extrayendo datos de p√°gina {page_number}...")
            
            # JavaScript para extraer datos del DOM
            js_extraction_script = """
            var properties = [];
            var postings = document.querySelectorAll('[data-qa="posting PROPERTY"]');
            
            postings.forEach(function(posting, index) {
                var property = {
                    index: index + 1,
                    full_text: posting.textContent.replace(/\\s+/g, ' ').trim()
                };
                
                // Intentar extraer elementos espec√≠ficos
                var selectors = {
                    price: ['.price', '[data-qa="price"]', '.posting-price', '.card-price', '.amount'],
                    location: ['.address', '[data-qa="address"]', '.posting-address', '.card-address', '.location'],
                    surface: ['.surface', '[data-qa="surface"]', '.posting-surface', '.card-surface', '.area'],
                    rooms: ['.rooms', '[data-qa="rooms"]', '.posting-rooms', '.card-rooms', '.ambientes']
                };
                
                Object.keys(selectors).forEach(function(field) {
                    for (var i = 0; i < selectors[field].length; i++) {
                        var elem = posting.querySelector(selectors[field][i]);
                        if (elem && elem.textContent.trim()) {
                            property[field] = elem.textContent.trim();
                            break;
                        }
                    }
                });
                
                properties.push(property);
            });
            
            return properties;
            """
            
            # Ejecutar JavaScript
            js_results = sb.execute_script(js_extraction_script)
            self.logger.info(f"üìã JavaScript extrajo {len(js_results)} elementos")
            
            # Procesar resultados con regex
            page_properties = []
            
            for i, js_prop in enumerate(js_results[:max_properties]):
                
                property_data = {
                    'id': f"prop_{page_number}_{i+1}",
                    'page': page_number,
                    'position': i + 1,
                    'scraping_date': datetime.now().isoformat(),
                    'extraction_method': 'cdp_javascript_regex'
                }
                
                full_text = js_prop.get('full_text', '')
                
                # Extraer precio con m√∫ltiples patrones
                price = self._extract_price(js_prop.get('price', ''), full_text)
                property_data['price'] = price
                
                # Extraer ubicaci√≥n
                location = self._extract_location(js_prop.get('location', ''), full_text)
                property_data['location'] = location
                
                # Extraer superficie
                surface = self._extract_surface(js_prop.get('surface', ''), full_text)
                property_data['surface'] = surface
                
                # Extraer habitaciones
                rooms = self._extract_rooms(js_prop.get('rooms', ''), full_text)
                property_data['rooms'] = rooms
                
                # Extraer informaci√≥n adicional
                property_data.update(self._extract_additional_info(full_text))
                
                # Solo agregar si tiene datos v√°lidos
                if self._is_valid_property(property_data):
                    page_properties.append(property_data)
            
            self.logger.info(f"‚úÖ Extra√≠das {len(page_properties)} propiedades v√°lidas de p√°gina {page_number}")
            return page_properties
            
        except Exception as e:
            self.logger.error(f"‚ùå Error extrayendo propiedades de p√°gina {page_number}: {e}")
            return []
    
    def _extract_price(self, direct_price: str, full_text: str) -> str:
        """Extrae precio usando m√∫ltiples estrategias"""
        if direct_price and direct_price.strip() and '$' in direct_price:
            return direct_price.strip()
        
        # Patrones de precio
        patterns = [
            r'\$\s*[\d,\.]+',          # $123,456
            r'USD\s*[\d,\.]+',         # USD 123,456
            r'[\d,\.]+\s*USD',         # 123,456 USD
            r'\$\s*[\d,\.]+\s*USD',    # $ 123,456 USD
        ]
        
        for pattern in patterns:
            match = re.search(pattern, full_text)
            if match:
                return match.group(0).strip()
        
        return "N/A"
    
    def _extract_location(self, direct_location: str, full_text: str) -> str:
        """Extrae ubicaci√≥n usando m√∫ltiples estrategias"""
        if direct_location and direct_location.strip() and len(direct_location) > 3:
            return direct_location.strip()
        
        # Patrones de ubicaci√≥n
        patterns = [
            r'[A-Z][a-z]+\s*,\s*[A-Z][a-z]+',     # Palermo, CABA
            r'[A-Z][a-z]+\s+[A-Z][a-z]+',         # Villa Crespo
            r'[A-Z][a-z]+\s*,\s*Capital',         # Recoleta, Capital
            r'[A-Z][a-z]+\s*,\s*Buenos Aires',    # Tigre, Buenos Aires
        ]
        
        for pattern in patterns:
            match = re.search(pattern, full_text)
            if match:
                location = match.group(0).strip()
                if len(location) > 5:  # Filtrar matches muy cortos
                    return location
        
        return "N/A"
    
    def _extract_surface(self, direct_surface: str, full_text: str) -> str:
        """Extrae superficie usando m√∫ltiples estrategias"""
        if direct_surface and 'm' in direct_surface:
            return direct_surface.strip()
        
        # Patrones de superficie
        patterns = [
            r'\d+\s*m¬≤\s*tot',        # 80 m¬≤ tot
            r'\d+\s*m¬≤',              # 80 m¬≤
            r'\d+\s*m2',              # 80 m2
            r'\d+\s*mts¬≤',            # 80 mts¬≤
            r'\d+\s*metros',          # 80 metros
        ]
        
        for pattern in patterns:
            match = re.search(pattern, full_text)
            if match:
                return match.group(0).strip()
        
        return "N/A"
    
    def _extract_rooms(self, direct_rooms: str, full_text: str) -> str:
        """Extrae habitaciones usando m√∫ltiples estrategias"""
        if direct_rooms and ('amb' in direct_rooms or 'dorm' in direct_rooms):
            return direct_rooms.strip()
        
        # Patrones de habitaciones
        patterns = [
            r'\d+\s*amb\.',           # 3 amb.
            r'\d+\s*amb',             # 3 amb
            r'\d+\s*dorm\.',          # 2 dorm.
            r'\d+\s*dorm',            # 2 dorm
            r'\d+\s*habitac',         # 2 habitaciones
            r'\d+\s*ambientes',       # 3 ambientes
        ]
        
        for pattern in patterns:
            match = re.search(pattern, full_text, re.IGNORECASE)
            if match:
                return match.group(0).strip()
        
        return "N/A"
    
    def _extract_additional_info(self, full_text: str) -> Dict[str, str]:
        """Extrae informaci√≥n adicional como ba√±os, cocheras, etc."""
        additional = {}
        
        # Ba√±os
        bath_match = re.search(r'(\d+)\s*ba√±o', full_text, re.IGNORECASE)
        if bath_match:
            additional['bathrooms'] = bath_match.group(0)
        
        # Cocheras
        garage_match = re.search(r'(\d+)\s*coch', full_text, re.IGNORECASE)
        if garage_match:
            additional['garage'] = garage_match.group(0)
        
        # Expensas
        expenses_match = re.search(r'\$\s*[\d,\.]+\s*Expensas', full_text)
        if expenses_match:
            additional['expenses'] = expenses_match.group(0)
        
        return additional
    
    def _is_valid_property(self, property_data: Dict[str, Any]) -> bool:
        """Verifica si la propiedad tiene datos v√°lidos"""
        required_fields = ['price', 'location', 'surface', 'rooms']
        valid_count = sum(1 for field in required_fields 
                         if property_data.get(field) and property_data.get(field) != 'N/A')
        
        # Considerar v√°lida si tiene al menos 2 campos v√°lidos
        return valid_count >= 2
    
    def save_data(self, filename: str = "zonaprop_full_scraping", format: str = "both") -> Dict[str, str]:
        """
        Guarda los datos extra√≠dos
        
        Args:
            filename: Nombre base del archivo
            format: 'csv', 'json', o 'both'
            
        Returns:
            Dict con las rutas de los archivos guardados
        """
        if not self.scraped_data:
            self.logger.warning("‚ö†Ô∏è  No hay datos para guardar")
            return {}
        
        # Crear directorio de salida
        output_dir = Path("data/raw")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        # Guardar CSV
        if format in ['csv', 'both']:
            csv_file = output_dir / f"{filename}_{timestamp}.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                if self.scraped_data:
                    # Obtener todos los campos √∫nicos de todas las propiedades
                    all_fields = set()
                    for prop in self.scraped_data:
                        all_fields.update(prop.keys())
                    
                    writer = csv.DictWriter(f, fieldnames=sorted(all_fields))
                    writer.writeheader()
                    writer.writerows(self.scraped_data)
            saved_files['csv'] = str(csv_file)
            self.logger.info(f"üíæ CSV guardado: {csv_file}")
        
        # Guardar JSON
        if format in ['json', 'both']:
            json_file = output_dir / f"{filename}_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.scraped_data, f, ensure_ascii=False, indent=2)
            saved_files['json'] = str(json_file)
            self.logger.info(f"üíæ JSON guardado: {json_file}")
        
        # Mostrar estad√≠sticas
        self._show_statistics()
        
        return saved_files
    
    def _show_statistics(self):
        """Muestra estad√≠sticas detalladas de los datos extra√≠dos"""
        if not self.scraped_data:
            return
        
        total = len(self.scraped_data)
        
        print(f"\nüìä ESTAD√çSTICAS DEL SCRAPING COMPLETO")
        print("=" * 60)
        print(f"Total de propiedades extra√≠das: {total}")
        
        # P√°ginas procesadas
        pages = sorted(set(prop.get('page', 0) for prop in self.scraped_data))
        print(f"P√°ginas procesadas: {pages}")
        print(f"Propiedades por p√°gina: {total // len(pages) if pages else 0} promedio")
        
        # Calidad de datos por campo
        fields = ['price', 'location', 'surface', 'rooms']
        print(f"\nüìã Calidad de datos:")
        
        for field in fields:
            valid_count = sum(1 for prop in self.scraped_data 
                            if prop.get(field) and prop.get(field) != 'N/A')
            percentage = (valid_count / total * 100) if total > 0 else 0
            print(f"  {field.capitalize()}: {valid_count}/{total} ({percentage:.1f}%)")
        
        # Rangos de precios
        prices = []
        for prop in self.scraped_data:
            price_text = prop.get('price', '').replace('$', '').replace(',', '').replace('.', '').replace('USD', '').strip()
            if price_text.isdigit():
                prices.append(int(price_text))
        
        if prices:
            print(f"\nüí∞ An√°lisis de precios:")
            print(f"  Rango: ${min(prices):,} - ${max(prices):,}")
            print(f"  Promedio: ${sum(prices) // len(prices):,}")
            print(f"  Propiedades con precio: {len(prices)}/{total}")
        
        # Ubicaciones √∫nicas
        locations = [prop.get('location', '') for prop in self.scraped_data 
                    if prop.get('location') and prop.get('location') != 'N/A']
        unique_locations = len(set(locations))
        print(f"\nüìç Ubicaciones √∫nicas: {unique_locations}")
        
        # Top 5 ubicaciones
        if locations:
            from collections import Counter
            top_locations = Counter(locations).most_common(5)
            print(f"  Top ubicaciones:")
            for location, count in top_locations:
                print(f"    {location}: {count} propiedades")


def main():
    """Funci√≥n principal para ejecutar el scraper desde l√≠nea de comandos"""
    parser = argparse.ArgumentParser(description="ZonaProp Full Scraper con CDP Mode")
    parser.add_argument("--pages", type=int, default=5, help="N√∫mero de p√°ginas a scrapear (default: 5)")
    parser.add_argument("--start-page", type=int, default=1, help="P√°gina inicial (default: 1)")
    parser.add_argument("--output", type=str, default="zonaprop_full", help="Nombre base del archivo de salida")
    parser.add_argument("--format", type=str, choices=["csv", "json", "both"], default="both", help="Formato de salida")
    parser.add_argument("--headless", action="store_true", help="Ejecutar en modo headless")
    parser.add_argument("--no-incognito", action="store_true", help="Desactivar modo inc√≥gnito")
    parser.add_argument("--delay", type=float, default=3.0, help="Delay entre p√°ginas en segundos (default: 3.0)")
    parser.add_argument("--properties-per-page", type=int, default=25, help="Propiedades por p√°gina (default: 25)")
    
    args = parser.parse_args()
    
    try:
        print("üöÄ ZonaProp Full Scraper - CDP Mode")
        print(f"üìÑ P√°ginas: {args.start_page} a {args.start_page + args.pages - 1}")
        print(f"üéØ Objetivo: ~{args.pages * args.properties_per_page} propiedades")
        print("=" * 60)
        
        # Crear instancia del scraper
        scraper = ZonaPropFullScraper(
            headless=args.headless,
            incognito=not args.no_incognito,
            delay=args.delay
        )
        
        # Ejecutar scraping
        data = scraper.scrape_multiple_pages(
            start_page=args.start_page,
            max_pages=args.pages,
            properties_per_page=args.properties_per_page
        )
        
        # Guardar datos
        if data:
            saved_files = scraper.save_data(args.output, args.format)
            
            print(f"\nüéâ SCRAPING COMPLETADO EXITOSAMENTE!")
            print(f"üìä Total extra√≠do: {len(data)} propiedades")
            
            if saved_files:
                print(f"üìÅ Archivos guardados:")
                for format_type, filepath in saved_files.items():
                    print(f"  {format_type.upper()}: {filepath}")
            
            print(f"\n‚úÖ CDP Mode: Bypass exitoso de Cloudflare")
            print(f"‚úÖ Extracci√≥n: Datos reales verificados")
            
        else:
            print("‚ùå No se extrajeron datos")
            return 1
            
    except KeyboardInterrupt:
        print("\nüõë Scraping interrumpido por el usuario")
        return 0
    except Exception as e:
        print(f"‚ùå Error en el scraping: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
